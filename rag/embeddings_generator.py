#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ðŸ§  Ð“Ð•ÐÐ•Ð ÐÐ¦Ð˜Ð¯ Ð­ÐœÐ‘Ð•Ð”Ð”Ð˜ÐÐ“ÐžÐ’ Ð”Ð›Ð¯ RAG

Ð­Ñ‚Ð¾Ñ‚ Ð¼Ð¾Ð´ÑƒÐ»ÑŒ ÑÐ¾Ð·Ð´Ð°ÐµÑ‚ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ (ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸) Ñ‡Ð°Ð½ÐºÐ¾Ð² Ñ‚ÐµÐºÑÑ‚Ð°
Ð´Ð»Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð² ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¿Ð¾ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¼Ñƒ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ñƒ.
ÐžÐ½ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Google Gemini API.

Ð—ÐÐŸÐ£Ð¡Ðš:
    python rag/embeddings_generator.py
"""

import json
import numpy as np
from pathlib import Path
from typing import Dict, List
import time
import os
import google.generativeai as genai
from dotenv import load_dotenv

class EmbeddingsGenerator:
    """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð´Ð»Ñ Ñ‡Ð°Ð½ÐºÐ¾Ð² Ñ‚ÐµÐºÑÑ‚Ð° Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Google Gemini API"""
    
    def __init__(self):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð¼Ð¾Ð´ÐµÐ»ÑŒ text-embedding-004.
        """
        self.model_name = "models/text-embedding-004"
        self.embedding_dim = 768  # Ð Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð´Ð»Ñ text-embedding-004
        print(f"Initialized generator with model: {self.model_name}")
        print(f"Embedding dimension: {self.embedding_dim}")
    
    def generate_embeddings(self, chunks_data: Dict[str, Dict[str, List[str]]], 
                          language: str = 'ru', batch_size: int = 100) -> Dict:
        """
        Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð´Ð»Ñ Ð²ÑÐµÑ… Ñ‡Ð°Ð½ÐºÐ¾Ð² Ñ‡ÐµÑ€ÐµÐ· Google Gemini API
        
        Args:
            chunks_data: ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ñ Ñ‡Ð°Ð½ÐºÐ°Ð¼Ð¸
            language: ÑÐ·Ñ‹Ðº ('ru' Ð¸Ð»Ð¸ 'en')
            batch_size: Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð° Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ (max 100 Ð´Ð»Ñ Gemini API)
            
        Returns:
            ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð°Ð¼Ð¸ Ð¸ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸
        """
        if batch_size > 100:
            print(f"WARNING: Batch size ({batch_size}) exceeds API limit (100). Setting to 100.")
            batch_size = 100

        embeddings_data = {
            'model': self.model_name,
            'embedding_dim': self.embedding_dim,
            'language': language,
            'books': {}
        }
        
        total_chunks = 0
        total_embeddings = 0
        
        # Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð²ÑÐµ Ñ‡Ð°Ð½ÐºÐ¸ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
        all_chunks_with_info = []
        
        for book_name in sorted(chunks_data.keys()):
            embeddings_data['books'][book_name] = {}
            
            for file_path in sorted(chunks_data[book_name].keys()):
                embeddings_data['books'][book_name][file_path] = []
                
                for chunk_idx, chunk_text in enumerate(chunks_data[book_name][file_path]):
                    all_chunks_with_info.append({
                        'text': chunk_text,
                        'book': book_name,
                        'file': file_path,
                        'chunk_idx': chunk_idx
                    })
                    total_chunks += 1
        
        print(f"Total chunks to process: {total_chunks:,}")
        print(f"Generating embeddings (batch_size={batch_size}). This may take a while...\n")
        
        # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð±Ð°Ñ‚Ñ‡Ð°Ð¼Ð¸
        start_time = time.time()
        
        for batch_start in range(0, len(all_chunks_with_info), batch_size):
            batch_end = min(batch_start + batch_size, len(all_chunks_with_info))
            batch_info = all_chunks_with_info[batch_start:batch_end]
            
            texts = [item['text'] for item in batch_info]
            
            try:
                # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ñ‡ÐµÑ€ÐµÐ· API
                result = genai.embed_content(
                    model=self.model_name,
                    content=texts,
                    task_type="RETRIEVAL_DOCUMENT" # ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
                )
                batch_embeddings = result['embedding']
                
                # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð² ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð´Ð°Ð½Ð½Ñ‹Ñ…
                for i, item in enumerate(batch_info):
                    embedding = batch_embeddings[i]
                    embeddings_data['books'][item['book']][item['file']].append({
                        'chunk_idx': item['chunk_idx'],
                        'text_preview': item['text'][:100],
                        'embedding': embedding
                    })
                    total_embeddings += 1

                # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ
                progress_pct = (batch_end / len(all_chunks_with_info)) * 100
                elapsed = time.time() - start_time
                rate = total_embeddings / elapsed if elapsed > 0 else 0
                eta = (len(all_chunks_with_info) - total_embeddings) / rate if rate > 0 else 0
                
                print(f"  {progress_pct:5.1f}% | {total_embeddings:7,} embeddings | {rate:5.1f} items/sec | ETA: {eta:6.0f}sec")

                # ÐŸÐ°ÑƒÐ·Ð°, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐ°Ñ‚ÑŒ Ð»Ð¸Ð¼Ð¸Ñ‚Ñ‹ API (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, 60 Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð² Ð¼Ð¸Ð½ÑƒÑ‚Ñƒ)
                time.sleep(1)

            except Exception as e:
                print(f"\nERROR processing batch {batch_start}-{batch_end}: {e}")
                print("   Skipping this batch. Check connection and API key.")
                continue

        elapsed = time.time() - start_time
        print(f"\nEmbeddings created in {elapsed:.1f} sec ({elapsed/60:.1f} min)")
        
        return embeddings_data
    
    def save_embeddings(self, embeddings_data: Dict, language: str = 'ru'):
        """
        Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð² Ñ„Ð°Ð¹Ð» (Ð² ÑÐ¶Ð°Ñ‚Ð¾Ð¼ Ð²Ð¸Ð´Ðµ Ñ NumPy)
        """
        # ... (ÑÑ‚Ð¾Ñ‚ Ð¼ÐµÑ‚Ð¾Ð´ Ð¾ÑÑ‚Ð°ÐµÑ‚ÑÑ Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹)
        output_file = f"rag/embeddings_{language}.npz"
        
        print(f"\nSaving embeddings to {output_file}...")
        
        embeddings_arrays = {}
        metadata = {
            'model': embeddings_data['model'],
            'embedding_dim': embeddings_data['embedding_dim'],
            'language': embeddings_data['language'],
            'structure': {}
        }
        
        idx = 0
        for book_name in sorted(embeddings_data['books'].keys()):
            metadata['structure'][book_name] = {}
            
            for file_path in sorted(embeddings_data['books'][book_name].keys()):
                chunk_list = embeddings_data['books'][book_name][file_path]
                
                if chunk_list:
                    embeddings_matrix = np.array([item['embedding'] for item in chunk_list])
                    key = f"embeddings_{idx}"
                    embeddings_arrays[key] = embeddings_matrix
                    
                    metadata['structure'][book_name][file_path] = {
                        'embedding_key': key,
                        'num_chunks': len(chunk_list),
                        'text_previews': [item['text_preview'] for item in chunk_list]
                    }
                    idx += 1
        
        np.savez_compressed(output_file, **embeddings_arrays)
        
        metadata_file = f"rag/embeddings_metadata_{language}.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        npz_size = Path(output_file).stat().st_size / (1024*1024)
        json_size = Path(metadata_file).stat().st_size / (1024*1024)
        
        print(f"NPZ file saved: {npz_size:.2f} MB")
        print(f"Metadata saved: {json_size:.2f} MB")
        
        return output_file, metadata_file

    def process_language(self, language: str = 'ru'):
        """
        ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð´Ð»Ñ Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ°
        """
        chunked_file = f"rag/chunked_scriptures_{language}.json"
        
        print(f"\nLoading chunks from {chunked_file}...")
        with open(chunked_file, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        
        print(f"Loaded {len(chunks_data)} books")
        
        embeddings_data = self.generate_embeddings(chunks_data, language=language, batch_size=100)
        
        if sum(len(file_data) for book_data in embeddings_data['books'].values() for file_data in book_data.values()) == 0:
            print("ERROR: No embeddings generated. Aborting.")
            return None

        npz_file, json_file = self.save_embeddings(embeddings_data, language=language)
        
        stats = {
            'language': language,
            'total_books': len(embeddings_data['books']),
            'embedding_model': embeddings_data['model'],
            'embedding_dim': embeddings_data['embedding_dim'],
            'npz_file': npz_file,
            'metadata_file': json_file
        }
        
        return stats


def process_all_languages():
    """ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð´Ð»Ñ Ð¾Ð±Ð¾Ð¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²"""
    
    import sys
    
    print("="*70)
    print("EMBEDDINGS GENERATOR - START (GOOGLE GEMINI API)")
    print("="*70)

    # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ API ÐºÐ»ÑŽÑ‡ Ð¸Ð· .env Ñ„Ð°Ð¹Ð»Ð°
    load_dotenv()
    if 'GEMINI_API_KEY' not in os.environ:
        print("ERROR: GEMINI_API_KEY environment variable not found.")
        return
    
    try:
        genai.configure(api_key=os.environ['GEMINI_API_KEY'])
        print("API Key configured.")
    except Exception as e:
        print(f"Error configuring API: {e}")
        return

    generator = EmbeddingsGenerator()
    
    all_stats = {}
    
    langs = []
    if len(sys.argv) > 1:
        arg = sys.argv[1].lower()
        if arg in ('ru', 'en'):
            langs = [arg]
        else:
            langs = ['ru', 'en']
    else:
        langs = ['ru', 'en']

    for lang in langs:
        print(f"\nSTAGE: {lang.upper()} SCRIPTURES")
        print("-" * 70)
        stats = generator.process_language(lang)
        if stats:
            all_stats[lang] = stats
    
    print("\n" + "="*70)
    if not all_stats:
        print("EMBEDDINGS GENERATION FAILED OR SKIPPED.")
    else:
        print("EMBEDDINGS GENERATION COMPLETED!")
    print("="*70)
    
    for lang, stats in all_stats.items():
        print(f"\n[STATS] {lang.upper()}:")
        print(f"   Books: {stats['total_books']}")
        print(f"   Model: {stats['embedding_model']}")
        print(f"   Dim: {stats['embedding_dim']}")
        print(f"   NPZ: {stats['npz_file']}")
        print(f"   Meta: {stats['metadata_file']}")
    
    if all_stats:
        print("\nNext step: Create index for fast search (FAISS)")
    
    return all_stats


if __name__ == "__main__":
    process_all_languages()
