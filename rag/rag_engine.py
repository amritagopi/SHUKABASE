"""
üß† RAG ENGINE - –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å Re-ranking –¥–ª—è Shukabase

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç:
1. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Google Gemini API
2. FAISS –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
3. Re-ranking —Å –ø–æ–º–æ—â—å—é Jina Reranker
4. –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
"""

import json
import numpy as np
import pickle
from pathlib import Path
from typing import List, Dict, Tuple, Any
import logging
import os
import time
import re

# –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
try:
    import faiss
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    import torch
    import google.generativeai as genai
    from dotenv import load_dotenv
    from rank_bm25 import BM25Okapi
    from nltk.stem import SnowballStemmer
except ImportError as e:
    raise ImportError(
        f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å: {e}. "
        "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–∫–µ—Ç—ã: pip install faiss-cpu transformers torch google-generativeai python-dotenv rank_bm25 nltk"
    )

logger = logging.getLogger(__name__)

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã (QueryExpander, RerankerModel) –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π ---

import difflib

class QueryExpander:
    """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    
    SYNONYMS_RU = {
        "–ª—é–±–æ–≤—å": ["–ø—Ä–µ–¥–∞–Ω–Ω–æ—Å—Ç—å", "–±—Ö–∞–∫—Ç–∏", "–¥—Ä—É–∂–±–∞", "–ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å", "prema"],
        "–±–æ–≥": ["–∫—Ä–∏—à–Ω–∞", "–≤–µ—Ä—Ö–æ–≤–Ω—ã–π", "–∞–±—Å–æ–ª—é—Ç", "–±–æ–∂–µ—Å—Ç–≤–æ", "–≤–∏—à–Ω—É", "–Ω–∞—Ä–∞—è–Ω–∞", "–≥–æ—Å–ø–æ–¥—å"],
        "–¥—É—à–∞": ["–∞—Ç–º–∞", "–¥—É—Ö", "—Å–æ–∑–Ω–∞–Ω–∏–µ", "—Å—É—â–Ω–æ—Å—Ç—å", "–¥–∂–∏–≤–∞"],
        "–∑–Ω–∞–Ω–∏–µ": ["–¥–∂–Ω—è–Ω–∞", "–º—É–¥—Ä–æ—Å—Ç—å", "–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "–æ—Å–æ–∑–Ω–∞–Ω–∏–µ", "–≤–µ–¥–∞"],
        "–π–æ–≥–∞": ["–ø—Ä–∞–∫—Ç–∏–∫–∞", "–º–µ–¥–∏—Ç–∞—Ü–∏—è", "–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞", "–ø—É—Ç—å", "—Å–∞–¥—Ö–∞–Ω–∞"],
        "–∫–∞—Ä–º–∞": ["–¥–µ–π—Å—Ç–≤–∏–µ", "–¥–µ—è–Ω–∏–µ", "—Å–ª–µ–¥—Å—Ç–≤–∏–µ", "—Å—É–¥—å–±–∞", "–∫–∞—Ä–º–∏—á–µ—Å–∫–∏–π"],
        "–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ": ["–º–æ–∫—à–∞", "—Å–ø–∞—Å–µ–Ω–∏–µ", "—Å–≤–æ–±–æ–¥–∞", "–≤—ã—Ö–æ–¥", "–Ω–∏—Ä–≤–∞–Ω–∞"],
        "–º–∏—Ä": ["–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–π", "–≤—Å–µ–ª–µ–Ω–Ω–∞—è", "–≤—Ä–µ–º–µ–Ω–Ω—ã–π", "–ø—Ä–µ—Ö–æ–¥—è—â–∏–π", "–º–∞–π—è", "–∏–ª–ª—é–∑–∏—è"],
        "–≥—É–Ω–∞": ["–∫–∞—á–µ—Å—Ç–≤–æ", "—Å–≤–æ–π—Å—Ç–≤–æ", "–ø—Ä–∏—Ä–æ–¥–∞", "—Å–∞—Ç—Ç–≤–∞", "—Ä–∞–¥–∂–∞—Å", "—Ç–∞–º–∞—Å"],
        "–ø—Ä–µ–¥–∞–Ω–Ω—ã–π": ["–≤–∞–π—à–Ω–∞–≤", "–±—Ö–∞–∫—Ç–∞", "—Å–ª—É–≥–∞", "—Å–∞–¥—Ö—É"],
        "—É—á–∏—Ç–µ–ª—å": ["–≥—É—Ä—É", "–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫", "–∞—á–∞—Ä—å—è", "—Å–≤–∞–º–∏", "–ø—Ä–∞–±—Ö—É–ø–∞–¥–∞"]
    }
    
    SYNONYMS_EN = {
        "love": ["devotion", "bhakti", "affection", "attachment", "prema"],
        "god": ["krishna", "supreme", "absolute", "deity", "vishnu", "narayana", "lord"],
        "soul": ["atma", "spirit", "consciousness", "essence", "jiva"],
        "knowledge": ["jnana", "wisdom", "understanding", "realization", "veda"],
        "yoga": ["practice", "meditation", "discipline", "path", "sadhana"],
        "karma": ["action", "deed", "consequence", "fate"],
        "liberation": ["moksha", "salvation", "freedom", "release", "nirvana"],
        "world": ["material", "universe", "temporary", "transient", "maya", "illusion"],
        "mode": ["guna", "quality", "nature", "sattva", "rajas", "tamas"],
        "devotee": ["vaishnava", "bhakta", "servant", "sadhu"],
        "teacher": ["guru", "master", "acharya", "swami", "prabhupada"]
    }
    
    @staticmethod
    def _fuzzy_find(term: str, collection: List[str], cutoff: float = 0.8) -> List[str]:
        return difflib.get_close_matches(term, collection, n=1, cutoff=cutoff)

    @staticmethod
    def expand_query_ru(query: str) -> List[str]:
        expanded = {query}
        query_words = query.lower().split()
        
        for word in query_words:
            # 1. Check keys
            for key, synonyms in QueryExpander.SYNONYMS_RU.items():
                # Direct or fuzzy match with key
                if key == word or QueryExpander._fuzzy_find(word, [key]):
                    expanded.add(key)
                    expanded.update(synonyms)
                
                # 2. Check values (synonyms)
                # Direct or fuzzy match with any synonym
                if word in synonyms or QueryExpander._fuzzy_find(word, synonyms):
                    expanded.add(key)
                    expanded.update(synonyms)
                    
        return list(expanded)[:5] # Limit to 5 variations to avoid token explosion
    
    @staticmethod
    def expand_query_en(query: str) -> List[str]:
        expanded = {query}
        query_words = query.lower().split()
        
        for word in query_words:
            # 1. Check keys
            for key, synonyms in QueryExpander.SYNONYMS_EN.items():
                if key == word or QueryExpander._fuzzy_find(word, [key]):
                    expanded.add(key)
                    expanded.update(synonyms)
                
                # 2. Check values
                if word in synonyms or QueryExpander._fuzzy_find(word, synonyms):
                    expanded.add(key)
                    expanded.update(synonyms)
                    
        return list(expanded)[:5]


class RerankerModel:
    """–ú–æ–¥–µ–ª—å re-ranking –¥–ª—è –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
    
    def __init__(self, model_name: str = "jinaai/jina-reranker-v2-base-multilingual"):
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å re-ranking: {model_name}")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_name, trust_remote_code=True, torch_dtype=torch.float32
            )
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model.to(self.device)
            self.model.eval()
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å re-ranking –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (device: {self.device})")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ re-ranking: {e}")
            self.model = None
    
    def rerank(self, query: str, documents: List[str], top_k: int = 5) -> List[Tuple[int, float, str]]:
        if not self.model or not documents:
            return [(i, 1.0, doc) for i, doc in enumerate(documents)][:top_k]
        try:
            with torch.no_grad():
                inputs = self.tokenizer(
                    [[query, doc] for doc in documents],
                    padding=True, truncation=True, return_tensors="pt", max_length=512
                ).to(self.device)
                scores = self.model(**inputs, return_dict=True).logits.squeeze(-1).cpu().numpy()
            
            ranked = sorted([(i, score, documents[i]) for i, score in enumerate(scores)], key=lambda x: x[1], reverse=True)
            return ranked[:top_k]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ re-ranking: {e}")
            return [(i, 1.0, doc) for i, doc in enumerate(documents)][:top_k]


# --- –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π RAGEngine ---

class RAGEngine:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å RAG —Å–∏—Å—Ç–µ–º—ã —Å Google Gemini API"""
    
    def __init__(
        self,
        reranker_model: str = "jinaai/jina-reranker-v2-base-multilingual",
        languages: List[str] = ['ru', 'en'],
        base_dir: str = "rag"
    ):
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é RAG Engine...")
        
        self._configure_gemini_api()
        
        self.base_dir = Path(base_dir)
        self.embedding_model_name = "models/text-embedding-004"
        self.languages = languages
        
        self.reranker = RerankerModel(reranker_model)
        
        self.stemmers = {
            'ru': SnowballStemmer('russian'),
            'en': SnowballStemmer('english')
        }
        
        self.indices: Dict[str, faiss.Index] = {}
        self.bm25_indices: Dict[str, Any] = {}
        self.metadata: Dict[str, Any] = {}
        self.chunked_data: Dict[str, Dict] = {}
        
        for lang in languages:
            self._load_language_data(lang)
        
        logger.info("‚úÖ RAG Engine –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")

    def _configure_gemini_api(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∫–ª—é—á API –¥–ª—è Gemini."""
        load_dotenv()
        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        try:
            genai.configure(api_key=api_key)
            logger.info("‚úÖ –ö–ª—é—á Gemini API —É—Å–ø–µ—à–Ω–æ —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω.")
        except Exception as e:
            raise RuntimeError(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Gemini API: {e}")

    def _load_language_data(self, language: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —á–∞–Ω–∫–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞."""
        index_file = self.base_dir / f"faiss_index_{language}.bin"
        metadata_file = self.base_dir / f"faiss_metadata_{language}.json"
        chunks_file = self.base_dir / f"chunked_scriptures_{language}.json"

        if not index_file.exists():
            logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å FAISS –Ω–µ –Ω–∞–π–¥–µ–Ω: {index_file}")
            return
            
        logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –¥–ª—è —è–∑—ã–∫–∞ '{language}'...")
        self.indices[language] = faiss.read_index(str(index_file))
        logger.info(f"  - –ó–∞–≥—Ä—É–∂–µ–Ω–æ {self.indices[language].ntotal:,} –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ {index_file}")

        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                raw_metadata = json.load(f)
            
            # Flatten metadata to match FAISS indices
            flat_metadata = []
            structure = raw_metadata.get('structure', {})
            
            # Collect all chapters
            all_chapters = []
            for book_key, book_data in structure.items():
                for chapter_key, chapter_data in book_data.items():
                    if 'embedding_key' in chapter_data:
                        all_chapters.append({
                            'book': book_key,
                            'chapter': chapter_key,
                            'data': chapter_data
                        })
            
            # Sort by embedding_key index (e.g., embeddings_0, embeddings_1)
            def get_embedding_index(item):
                key = item['data']['embedding_key']
                try:
                    return int(key.split('_')[1])
                except (IndexError, ValueError):
                    return 999999
            
            all_chapters.sort(key=get_embedding_index)
            
            # Create flat list
            for item in all_chapters:
                book = item['book']
                chapter = item['chapter']
                data = item['data']
                num_chunks = data.get('num_chunks', 0)
                text_previews = data.get('text_previews', [])
                
                for i in range(num_chunks):
                    preview = text_previews[i] if i < len(text_previews) else ""
                    flat_metadata.append({
                        'book': book,
                        'chapter': chapter,
                        'chunk_idx': i,
                        'text_preview': preview
                    })
            
            self.metadata[language] = flat_metadata
            logger.info(f"  - –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ ({len(flat_metadata)} –∑–∞–ø–∏—Å–µ–π)")
        else:
            logger.warning(f"  - –§–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {metadata_file}")

        if chunks_file.exists():
            with open(chunks_file, 'r', encoding='utf-8') as f:
                self.chunked_data[language] = json.load(f)
            logger.info(f"  - –ó–∞–≥—Ä—É–∂–µ–Ω—ã —á–∞–Ω–∫–∏ –∏–∑ {chunks_file}")
        else:
             logger.warning(f"  - –§–∞–π–ª —Å —á–∞–Ω–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {chunks_file}")

        # --- –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–ª–∏ –ó–∞–≥—Ä—É–∑–∫–∞ BM25 –∏–Ω–¥–µ–∫—Å–∞ ---
        bm25_file = self.base_dir / f"bm25_index_{language}.pkl"

        if language in self.metadata and self.metadata[language]:
            # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å
            if bm25_file.exists():
                logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –∏–Ω–¥–µ–∫—Å BM25 –¥–ª—è —è–∑—ã–∫–∞ '{language}' –∏–∑ —Ñ–∞–π–ª–∞...")
                try:
                    with open(bm25_file, 'rb') as f:
                        self.bm25_indices[language] = pickle.load(f)
                    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å BM25 —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ BM25 –∏–Ω–¥–µ–∫—Å–∞: {e}. –ë—É–¥—É —Å—Ç—Ä–æ–∏—Ç—å –∑–∞–Ω–æ–≤–æ.")

            # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω (—Ñ–∞–π–ª–∞ –Ω–µ—Ç –∏–ª–∏ –æ—à–∏–±–∫–∞), —Å—Ç—Ä–æ–∏–º –µ–≥–æ
            if language not in self.bm25_indices:
                logger.info(f"‚è≥ –°—Ç—Ä–æ—é –∏–Ω–¥–µ–∫—Å BM25 –¥–ª—è —è–∑—ã–∫–∞ '{language}'...")
                try:
                    corpus = []
                    for meta in self.metadata[language]:
                        text = self._get_text_from_meta(meta, language)
                        corpus.append(self._tokenize(text, language))
                    
                    self.bm25_indices[language] = BM25Okapi(corpus)
                    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å BM25 –ø–æ—Å—Ç—Ä–æ–µ–Ω ({len(corpus)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ä–∞–∑–∞
                    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—è—é –∏–Ω–¥–µ–∫—Å BM25 –≤ —Ñ–∞–π–ª {bm25_file}...")
                    with open(bm25_file, 'wb') as f:
                        pickle.dump(self.bm25_indices[language], f)
                    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å BM25 —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ BM25: {e}")

    def _get_embedding(self, texts: List[str]) -> np.ndarray:
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é Gemini API."""
        try:
            # RETRIEVAL_QUERY –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            if len(texts) == 1:
                # –î–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–∏–Ω–æ—á–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
                result = genai.embed_content(
                    model=self.embedding_model_name,
                    content=texts[0],
                    task_type="RETRIEVAL_QUERY"
                )
                embedding = result['embedding']
                return np.array([embedding], dtype='float32')
            else:
                # –î–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤
                all_embeddings = []
                for text in texts:
                    result = genai.embed_content(
                        model=self.embedding_model_name,
                        content=text,
                        task_type="RETRIEVAL_QUERY"
                    )
                    all_embeddings.append(result['embedding'])
                return np.array(all_embeddings, dtype='float32')
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –æ—Ç Gemini API: {e}", exc_info=True)
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–∞–¥–µ–Ω–∏—è
            dim = 768 # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è text-embedding-004
            return np.zeros((len(texts), dim), dtype='float32')

    def _tokenize(self, text: str, language: str) -> List[str]:
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å–æ —Å—Ç–µ–º–º–∏–Ω–≥–æ–º –¥–ª—è BM25"""
        words = re.findall(r'\w+', text.lower())
        stemmer = self.stemmers.get(language)
        if stemmer:
            return [stemmer.stem(w) for w in words]
        return words

    def _get_text_from_meta(self, meta: Dict, language: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º"""
        book = meta.get('book')
        chapter = meta.get('chapter')
        chunk_idx = meta.get('chunk_idx')
        
        text = ""
        chunks_map = self.chunked_data.get(language, {})
        
        if book and chapter and book in chunks_map and chapter in chunks_map[book]:
            chapter_chunks = chunks_map[book][chapter]
            if isinstance(chapter_chunks, list) and isinstance(chunk_idx, int):
                if 0 <= chunk_idx < len(chapter_chunks):
                    text = chapter_chunks[chunk_idx]
        
        if not text:
            text = meta.get('text_preview', '')
            
        return text

    def _search_by_keyword(self, query: str, language: str, top_k: int) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å –ø–æ–º–æ—â—å—é BM25"""
        bm25 = self.bm25_indices.get(language)
        if not bm25: return []
        
        try:
            tokenized_query = self._tokenize(query, language)
            scores = bm25.get_scores(tokenized_query)
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            top_n_indices = np.argsort(scores)[::-1][:top_k]
            
            results = []
            metadata_list = self.metadata.get(language, [])
            
            for idx in top_n_indices:
                score = scores[idx]
                if score <= 0: continue
                
                meta = metadata_list[idx] if idx < len(metadata_list) else {}
                text = self._get_text_from_meta(meta, language)
                
                results.append({
                    'index': int(idx),
                    'distance': 0.0,
                    'score': float(score), # Raw BM25 score
                    'text': text,
                    'book': meta.get('book'), 
                    'chapter': meta.get('chapter'), 
                    'verse': None, 
                    'chunk_idx': meta.get('chunk_idx'),
                    'html_path': meta.get('html_path'),
                    'source': 'bm25'
                })
            
            return results
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ keyword –ø–æ–∏—Å–∫–µ: {e}")
            return []


    def _search_by_vector(self, query_embedding: np.ndarray, language: str, top_k: int, vector_distance_threshold: float = None) -> List[Dict[str, Any]]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ FAISS."""
        index = self.indices.get(language)
        if not index: return []

        try:
            query_norm = query_embedding.copy().reshape(1, -1)
            faiss.normalize_L2(query_norm)
            distances, indices_found = index.search(query_norm, top_k * 2) # –ò—â–µ–º —Å –∑–∞–ø–∞—Å–æ–º
            
            results = []
            metadata_list = self.metadata.get(language, [])
            chunks_map = self.chunked_data.get(language, {})
            
            seen_ids = set()
            
            for i, (dist, idx) in enumerate(zip(distances[0], indices_found[0])):
                if idx < 0: continue

                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
                if vector_distance_threshold is not None and dist > vector_distance_threshold:
                    continue


                meta = metadata_list[idx] if isinstance(metadata_list, list) and idx < len(metadata_list) else {}
                book, chapter = meta.get('book'), meta.get('chapter')
                chunk_idx = meta.get('chunk_idx')
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
                unique_id = f"{book}_{chapter}_{chunk_idx}"
                if unique_id in seen_ids:
                    continue
                seen_ids.add(unique_id)
                
                # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
                text = self._get_text_from_meta(meta, language)
                if not text:
                    text = meta.get('text_preview', '') + '...'

                results.append({
                    'index': int(idx),
                    'distance': float(dist),
                    'score': float(1.0 / (1.0 + dist)),
                    'text': text,
                    'book': book, 
                    'chapter': chapter, 
                    'verse': None, 
                    'chunk_idx': chunk_idx,
                    'html_path': meta.get('html_path'),
                    'source': 'vector'
                })
                
                if len(results) >= top_k:
                    break

            return results
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ –≤–µ–∫—Ç–æ—Ä—É ({language}): {e}", exc_info=True)
            return []

    def _detect_verse_reference(self, query: str) -> Dict[str, Any]:
        """
        –ü—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å —Å—Å—ã–ª–∫–æ–π –Ω–∞ —Å—Ç–∏—Ö.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç {'book': 'bg', 'chapter': '2', 'verse': '13'} –∏–ª–∏ None.
        """
        query = query.lower().strip()
        
        # –ö–∞—Ä—Ç–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
        book_map = {
            'bg': 'bg', '–±–≥': 'bg', 'gita': 'bg', '–≥–∏—Ç–∞': 'bg', 'bhagavad': 'bg', 'bhagavad gita': 'bg', '–±—Ö–∞–≥–∞–≤–∞–¥ –≥–∏—Ç–∞': 'bg',
            'sb': 'sb', '—à–±': 'sb', 'bhagavatam': 'sb', '–±—Ö–∞–≥–∞–≤–∞—Ç–∞–º': 'sb', 'srimad bhagavatam': 'sb', '—à—Ä–∏–º–∞–¥ –±—Ö–∞–≥–∞–≤–∞—Ç–∞–º': 'sb',
            'cc': 'cc', '—á—á': 'cc', 'caitanya': 'cc', '—á–∞–π—Ç–∞–Ω—å—è': 'cc', 'caitanya caritamrta': 'cc', '—á–∞–π—Ç–∞–Ω—å—è —á–∞—Ä–∏—Ç–∞–º—Ä–∏—Ç–∞': 'cc',
            'iso': 'iso', '–∏—à–æ': 'iso', 'isopanisad': 'iso', 'sri isopanisad': 'iso', '—à—Ä–∏ –∏—à–æ–ø–∞–Ω–∏—à–∞–¥': 'iso',
            'nod': 'nod', '–Ω–ø': 'nod', 'nectar of devotion': 'nod',
            'noi': 'noi', '–Ω–Ω': 'noi', 'nectar of instruction': 'noi'
        }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω: (book) (chapter).(verse) –∏–ª–∏ (book) (chapter) (verse)
        # –ü—Ä–∏–º–µ—Ä: –ë–≥ 2.13, SB 1.1.1, CC Adi 1.1, Bhagavad Gita 2.13
        
        # 1. –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω: Book Chapter.Verse
        # –†–∞–∑—Ä–µ—à–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –∫–Ω–∏–≥–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Bhagavad Gita)
        match = re.search(r'([a-z–∞-—è\s]+?)\.?\s*(\d+)[. :](\d+)', query)
        if match:
            book_raw, chapter, verse = match.groups()
            book_key = book_raw.strip()
            if book_key in book_map:
                return {'book': book_map[book_key], 'chapter': chapter, 'verse': verse}
        
        # 2. –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è SB: 1.1.1 (Canto.Chapter.Verse)
        match_sb = re.search(r'([a-z–∞-—è\s]+?)\.?\s*(\d+)\.(\d+)\.(\d+)', query)
        if match_sb:
            book_raw, canto, chapter, verse = match_sb.groups()
            book_key = book_raw.strip()
            if book_key in book_map:
                return {'book': book_map[book_key], 'chapter': f"{canto}.{chapter}", 'verse': verse}

        return None

    def _find_verse_in_metadata(self, ref: Dict[str, Any], language: str) -> List[Dict[str, Any]]:
        """–ò—â–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Å—Ç–∏—Ö –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö."""
        results = []
        metadata_list = self.metadata.get(language, [])
        
        target_book = ref['book']
        target_chapter = ref['chapter'] # –ú–æ–∂–µ—Ç –±—ã—Ç—å "2" –∏–ª–∏ "1.1"
        target_verse = ref['verse']
        
        logger.info(f"üéØ –ò—â—É —Å—Ç–∏—Ö: Book={target_book}, Chapter={target_chapter}, Verse={target_verse}")
        
        for idx, meta in enumerate(metadata_list):
            if meta.get('book') == target_book:
                # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥–ª–∞–≤. –í –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å "2" –∏–ª–∏ "02".
                meta_chapter = str(meta.get('chapter', ''))
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–∞–≤ (—É–±–∏—Ä–∞–µ–º –≤–µ–¥—É—â–∏–µ –Ω—É–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
                def normalize_chapter(ch):
                    return '.'.join([p.lstrip('0') for p in str(ch).split('.')])
                
                if normalize_chapter(meta_chapter) == normalize_chapter(target_chapter):
                    
                    text = self._get_text_from_meta(meta, language)
                    
                    # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: —Å—Ç–∏—Ö –æ–±—ã—á–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –Ω–æ–º–µ—Ä–∞ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –µ–≥–æ –≤ –Ω–∞—á–∞–ª–µ
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –Ω–æ–º–µ—Ä —Å—Ç–∏—Ö–∞ –≤ –Ω–∞—á–∞–ª–µ —Ç–µ–∫—Å—Ç–∞ (–∏–ª–∏ –≤ –ø–µ—Ä–≤—ã—Ö 20 —Å–∏–º–≤–æ–ª–∞—Ö)
                    # TEXT-13 ... –∏–ª–∏ 13. ...
                    
                    # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –º–∞—Ä–∫–µ—Ä–æ–≤ —Ç–∏–ø–∞ TEXT 13
                    clean_text = text.lower()
                    
                    # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ —Å—Ç–∏—Ö–∞
                    # –í–∞—Ä–∏–∞–Ω—Ç—ã: "13.", "text 13", "—Ç–µ–∫—Å—Ç 13"
                    is_match = False
                    
                    if f"text {target_verse}" in clean_text[:50]:
                        is_match = True
                    elif f"—Ç–µ–∫—Å—Ç {target_verse}" in clean_text[:50]:
                        is_match = True
                    elif clean_text.strip().startswith(f"{target_verse}."):
                        is_match = True
                    # –î–ª—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Å—Ç–∏—Ö–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä 13-14)
                    elif f"{target_verse}-" in clean_text[:20]:
                        is_match = True
                        
                    if is_match:
                        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ç–æ—á–Ω—ã–π —Å—Ç–∏—Ö –≤ –∏–Ω–¥–µ–∫—Å–µ {idx}")
                        results.append({
                            'index': int(idx),
                            'distance': 0.0,
                            'score': 100.0, # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä
                            'text': text,
                            'book': target_book, 
                            'chapter': meta_chapter, 
                            'verse': target_verse, 
                            'chunk_idx': meta.get('chunk_idx'),
                            'html_path': meta.get('html_path'),
                            'source': 'exact_verse'
                        })
        
        return results

    def search(
        self, 
        query: str, 
        language: str = 'ru', 
        top_k: int = 5, 
        use_reranking: bool = True,
        expand_query: bool = True,
        vector_distance_threshold: float = None
    ) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞."""
        logger.info(f"üîç –ü–æ–∏—Å–∫: '{query}' ({language}, top_k={top_k})")
        if language not in self.indices:
            return {'success': False, 'error': f'–ò–Ω–¥–µ–∫—Å –¥–ª—è —è–∑—ã–∫–∞ {language} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω.'}

        try:
            # 0. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–æ—á–Ω—ã–π —Å—Ç–∏—Ö
            verse_ref = self._detect_verse_reference(query)
            exact_results = []
            if verse_ref:
                exact_results = self._find_verse_in_metadata(verse_ref, language)
                if exact_results:
                    logger.info(f"üéâ –ù–∞–π–¥–µ–Ω—ã —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å—Ç–∏—Ö–æ–≤: {len(exact_results)}")
                    return {
                        'success': True,
                        'results': exact_results,
                        'query': query,
                        'search_type': 'exact_verse_reference',
                        'count': len(exact_results)
                    }

            # 1. –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
            query_variants = [query]
            if expand_query:
                expander_method = getattr(QueryExpander, f'expand_query_{language}', None)
                if expander_method:
                    query_variants = expander_method(query)

            logger.info(f"   üìã –í–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–∞: {query_variants}")

            # 2. –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –æ–¥–Ω–∏–º –±–∞—Ç—á–µ–º
            variant_embeddings = self._get_embedding(query_variants)
            logger.info(f"   üî¢ –ü–æ–ª—É—á–µ–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {variant_embeddings.shape}")

            # 3. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞
            all_results = []
            for idx, emb in enumerate(variant_embeddings):
                vector_results = self._search_by_vector(emb, language, top_k * 2, vector_distance_threshold)
                # logger.info(f"   üîé –í–∞—Ä–∏–∞–Ω—Ç '{query_variants[idx]}': –Ω–∞–π–¥–µ–Ω–æ {len(vector_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                all_results.extend(vector_results)

            # 4. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –æ—Ç–±–æ—Ä –ª—É—á—à–∏—Ö –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            seen_indices = set()
            unique_vector_results = []
            for res in sorted(all_results, key=lambda x: x['score'], reverse=True):
                if res['index'] not in seen_indices:
                    seen_indices.add(res['index'])
                    unique_vector_results.append(res)

            top_vector_results = unique_vector_results[:top_k * 2] # –ë–µ—Ä–µ–º —Å –∑–∞–ø–∞—Å–æ–º –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è
            
            # 5. Keyword Search (BM25)
            keyword_results = []
            if language in self.bm25_indices:
                # logger.info(f"   üìö –ó–∞–ø—É—Å–∫ BM25 –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}'...")
                keyword_results = self._search_by_keyword(query, language, top_k * 2)
                # logger.info(f"      - –ù–∞–π–¥–µ–Ω–æ {len(keyword_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")

            # 6. Hybrid Fusion (RRF - Reciprocal Rank Fusion)
            # RRF score = 1 / (k + rank)
            k_rrf = 60
            combined_scores = {}
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –≤–µ—Å–æ–º
            for res in exact_results:
                idx = res['index']
                combined_scores[idx] = {'data': res, 'rrf_score': 100.0} # Super high score

            # Process Vector Results
            for rank, res in enumerate(top_vector_results):
                idx = res['index']
                if idx not in combined_scores:
                    combined_scores[idx] = {'data': res, 'rrf_score': 0.0}
                # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ —Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –¥–æ–±–∞–≤–ª—è–µ–º RRF
                if combined_scores[idx]['rrf_score'] < 50.0:
                    combined_scores[idx]['rrf_score'] += 1.0 / (k_rrf + rank + 1)
                    combined_scores[idx]['data']['vector_rank'] = rank + 1
                
            # Process Keyword Results
            for rank, res in enumerate(keyword_results):
                idx = res['index']
                if idx not in combined_scores:
                    combined_scores[idx] = {'data': res, 'rrf_score': 0.0}
                if combined_scores[idx]['rrf_score'] < 50.0:
                    combined_scores[idx]['rrf_score'] += 1.0 / (k_rrf + rank + 1)
                    combined_scores[idx]['data']['keyword_rank'] = rank + 1

            # Sort by RRF score
            hybrid_results = sorted(combined_scores.values(), key=lambda x: x['rrf_score'], reverse=True)
            
            # Extract top_k
            final_candidates = []
            for item in hybrid_results[:top_k]:
                res = item['data']
                res['score'] = item['rrf_score'] # –û–±–Ω–æ–≤–ª—è–µ–º —Å–∫–æ—Ä –Ω–∞ RRF
                final_candidates.append(res)
            
            logger.info(f"   ü§ù –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(final_candidates)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

            # 7. –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ (Re-ranking)
            # –ù–µ –¥–µ–ª–∞–µ–º —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥ –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (score > 50)
            if use_reranking and self.reranker.model:
                docs_to_rerank = []
                indices_to_rerank = []
                
                final_results = []
                
                for i, res in enumerate(final_candidates):
                    if res['score'] > 50.0:
                        # –≠—Ç–æ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å –Ω–∞ –ø–µ—Ä–≤–æ–º –º–µ—Å—Ç–µ
                        res['final_score'] = 1.0
                        final_results.append(res)
                    else:
                        docs_to_rerank.append(res['text'])
                        indices_to_rerank.append(i)
                
                if docs_to_rerank:
                    reranked_tuples = self.reranker.rerank(query, docs_to_rerank, len(docs_to_rerank))
                    
                    for original_idx_in_subset, score, text in reranked_tuples:
                        original_idx = indices_to_rerank[original_idx_in_subset]
                        original_result = final_candidates[original_idx]
                        original_result['final_score'] = float(score)
                        final_results.append(original_result)
            else:
                final_results = final_candidates

            return {
                'success': True,
                'results': final_results,
                'query_variants': query_variants,
                'count': len(final_results)
            }
        
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}", exc_info=True)
            return {'success': False, 'error': str(e), 'query': query}

    def keyword_search(self, query: str, language: str = 'en', case_sensitive: bool = False) -> Dict[str, Any]:
        """
        –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –í–°–ï —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            language: –Ø–∑—ã–∫ ('en' –∏–ª–∏ 'ru')
            case_sensitive: –£—á–∏—Ç—ã–≤–∞—Ç—å —Ä–µ–≥–∏—Å—Ç—Ä (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)
        
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞
        """
        import re
        
        logger.info(f"üîç Keyword search: '{query}' (language={language}, case_sensitive={case_sensitive})")
        
        if language not in self.languages:
            logger.error(f"–Ø–∑—ã–∫ {language} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
            return {'success': False, 'error': f'Language {language} not supported'}
        
        metadata = self.metadata[language]
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        search_query = query if case_sensitive else query.lower()
        
        results = []
        
        # –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —á–∞–Ω–∫–∞–º
        for item in metadata:
            text = self._get_text_from_meta(item, language)
            search_text = text if case_sensitive else text.lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            if search_query in search_text:
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
                occurrences = search_text.count(search_query)
                
                results.append({
                    'text': text,
                    'book': item.get('book', 'Unknown'),
                    'chapter': item.get('chapter', ''),
                    'verse': item.get('verse', ''),
                    'chunk_idx': item.get('chunk_idx', 0),
                    'page_number': item.get('page_number'),
                    'html_path': item.get('html_path'),
                    'occurrences': occurrences,
                    'score': min(1.0, occurrences / 10.0)  # –ü—Ä–æ—Å—Ç–æ–π score –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏–π
                })
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤—Ö–æ–∂–¥–µ–Ω–∏–π (–±–æ–ª—å—à–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–π = –≤—ã—à–µ –≤ —Å–ø–∏—Å–∫–µ)
        results.sort(key=lambda x: x['occurrences'], reverse=True)
        
        logger.info(f"‚úÖ Keyword search found {len(results)} results")
        
        return {
            'success': True,
            'results': results,
            'query': query,
            'total_results': len(results),
            'language': language
        }